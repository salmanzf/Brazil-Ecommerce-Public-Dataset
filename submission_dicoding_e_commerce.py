# -*- coding: utf-8 -*-
"""Submission_Dicoding_E-Commerce

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/192YpxyRiMIMRdGzGfCSCcOY364x3uXlk

# Data Analysis Project: E-Commerce Public Dataset
- Nama: Salman Zahid Fathurrahman
- Email: salmanzf@ymail.com
- Id Dicoding:
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Defining Business Question

- How is the efficient strategy to improve revenue?
- Bagaimana cara mengelola operation secara efektif untuk mengurangi cost of operation?

## Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime

from google.colab import drive
drive.mount('/content/drive')

"""## Data Wrangling

### Gathering Data
"""

df_order_item = pd.read_csv('drive/MyDrive/Portfolio_Ecommerce-Dicoding/olist_order_items_dataset.csv')
df_order_payment = pd.read_csv('drive/MyDrive/Portfolio_Ecommerce-Dicoding/olist_order_payments_dataset.csv')
df_product = pd.read_csv('drive/MyDrive/Portfolio_Ecommerce-Dicoding/olist_products_dataset.csv')
df_product_cat = pd.read_csv('drive/MyDrive/Portfolio_Ecommerce-Dicoding/product_category_name_translation.csv')
df_seller = pd.read_csv('drive/MyDrive/Portfolio_Ecommerce-Dicoding/olist_sellers_dataset.csv')
df_customer = pd.read_csv('drive/MyDrive/Portfolio_Ecommerce-Dicoding/olist_customers_dataset.csv')
df_review = pd.read_csv('drive/MyDrive/Portfolio_Ecommerce-Dicoding/olist_order_reviews_dataset.csv')
df_delivery = pd.read_csv('drive/MyDrive/Portfolio_Ecommerce-Dicoding/olist_orders_dataset.csv')

"""### Assessing Data

#### Merging Order Item-Order Payment-Product-Product Cat
"""

#Order Price = Order Item + Order Payment
df_order_price = df_order_item.merge(df_order_payment, how='inner', on='order_id').sort_values(by='order_id')
df_order_price.head(3)

#Product Category = Product + Product Cat
f_product_category = df_product.merge(df_product_cat, on='product_category_name', how='inner')[['product_id', 'product_category_name_english']]d
df_product_category.head(3)

#Order Product = Order Price + Product Category
df_order_product = df_order_price.merge(df_product_category, on='product_id', how='inner').groupby(by=['order_id', 'product_id', 'product_category_name_english'], as_index=False).sum()
df_order_product.head(3)

"""#### Merging Customer-Review-Delivery"""

#customer dataset
df_customer = df_customer[['customer_id', 'customer_unique_id', 'customer_city', 'customer_state']]
df_customer.head(3)

#review dataset
df_review = df_review[['review_id', 'order_id', 'review_score']]
df_review.head(3)

#delivery dataset
df_delivery.head(3)

#converting to datetima type
cols = df_delivery.columns[3:]
df_delivery[cols] = df_delivery[cols].apply(pd.to_datetime, errors='coerce')

df_delivery.dtypes

#creating various duration column

#duration from order to arrive at customer
df_delivery['duration_arrived'] = (df_delivery['order_delivered_customer_date']-df_delivery['order_purchase_timestamp']).dt.days

#duration from carrier to arrive at customer
df_delivery['duration_carrier_customer'] = (df_delivery['order_delivered_customer_date']-df_delivery['order_delivered_carrier_date']).dt.days

#duration from purchase to arrive at carrier
df_delivery['duration_customer_carrier'] = (df_delivery['order_delivered_carrier_date']-df_delivery['order_purchase_timestamp']).dt.days

#difference of estimated time to duration_arrived
df_delivery['estimated_qos'] = (df_delivery['order_estimated_delivery_date']-df_delivery['order_delivered_customer_date']).dt.days

df_delivery.head(3)

#timestamp dataset = modified delivery dataset
df_timestamp = df_delivery.iloc[:,[0,1,2,3,8,9,10,11]].sort_values('order_purchase_timestamp').reset_index(drop=True)
df_timestamp

"""### Cleaning Data

#### Timestamp Dataset
"""

#cleaning data mostly drom TimeStamp Dataset

#drop NA - for 'delivered' status only in duration
df_timestamp = df_timestamp.dropna()
df_timestamp.isna().sum()

#finding anomalies in duration_arrived
q1 = np.percentile(df_timestamp['duration_arrived'], 25)
q3 = np.percentile(df_timestamp['duration_arrived'], 75)
#threshold iqr
iqr = q3-q1
iqr_anomaly = 1.5*iqr
#threshold data q1 & q3
q1_anomaly = q1-iqr_anomaly
q3_anomaly = q3+iqr_anomaly

#drop outlier (outside threshold q1 & q3)
df_timestamp = df_timestamp[(df_timestamp['duration_arrived']>q1_anomaly) | (df_timestamp['duration_arrived']<q3_anomaly)].reset_index(drop=True)
df_timestamp

#2966 data dropped

"""## Exploratory Data Analysis (EDA)

### Revenue

#### By Product
"""

#melakukan penggabungan untuk produk dan transaksi
df_time_price = df_timestamp.merge(df_order_product, on='order_id', how='inner')
df_time_price

df_time_price = df_time_price[df_time_price['order_status'] == 'delivered']

df_time_price
#7 dropped

#Total Revenue
df_revenue_product = df_time_price.groupby('product_category_name_english', as_index=False)['payment_value'].sum().sort_values('payment_value', ascending=False).reset_index(drop=True)
df_revenue_product

sns.barplot(data=df_revenue_product.head(10), x='payment_value', y='product_category_name_english')

"""#### By Period"""

#create column for periods observation
df_time_price['order_purchase_year'] = df_time_price['order_purchase_timestamp'].dt.year
df_time_price['order_purchase_month-year'] = df_time_price['order_purchase_timestamp'].dt.to_period('M').dt.to_timestamp()

df_time_price.groupby(df_time_price['order_purchase_month-year'], as_index=False)['payment_value'].sum().sort_values('payment_value', ascending=False).reset_index(drop=True)

plt.figure(figsize=(20,10))
sns.lineplot(data=df_time_price, x='order_purchase_month-year', y='payment_value', estimator='sum', ci=None)

#2018-09 too inconsistent, drop later

"""#### By Area"""

revenue_area = df_time_price.merge(df_customer, on='customer_id', how='inner')
revenue_area.head(3)

"""##### By State"""

revenue_area_state = revenue_area.groupby('customer_state', as_index=False)['payment_value'].sum().sort_values('payment_value', ascending=False)

revenue_area_state['percentage'] = (revenue_area_state['payment_value']/revenue_area_state['payment_value'].sum())*100
revenue_area_state.head(3)

sns.barplot(data=revenue_area_state.head(10), x='payment_value', y='customer_state')

"""##### By City"""

revenue_area_city = revenue_area.groupby('customer_city', as_index=False)['payment_value'].sum().sort_values('payment_value', ascending=False).reset_index(drop=True)

revenue_area_city['percentage'] = round((revenue_area_city['payment_value']/revenue_area_city['payment_value'].sum())*100, 2)
revenue_area_city.head(10)

sns.barplot(data=revenue_area_city.head(10), x='payment_value', y='customer_city')

"""### Product Popularity (By Total Order)"""

df_product_popularity = df_timestamp.merge(df_order_product, on='order_id', how='inner')
df_product_popularity.head(5)

df_product_popularity[df_product_popularity['order_id'].duplicated(keep=False)]

df_product_popularity = df_product_popularity.groupby('product_category_name_english', as_index=False)['order_item_id'].sum().sort_values('order_item_id', ascending=False).reset_index(drop=True)
df_product_popularity.head(5)

#pretty similar results to the Revenue By Product

"""### Freight Value & Duration"""

#Since freight value is correlated to shipping cost, it also correlate with duration (hypothesis)
df_delivery['duration_estimation'] = (df_delivery['order_estimated_delivery_date']-df_delivery['order_purchase_timestamp']).dt.days
df_freight_estimation = df_delivery.merge(df_order_price, on='order_id', how='inner')
df_freight_estimation.head(3)

sns.scatterplot(data=df_freight_estimation, x='freight_value', y='duration_arrived')

#there is no definite correlation between freight value and duration

"""### Review"""

#Merge review & timestamp
review_timestamp = df_review.merge(df_timestamp, on='order_id', how='inner')
review_timestamp.head(5)

review_timestamp.isna().sum()

"""#### Review vs Duration Arrived"""

#mean review vs duration
review_vs_arrive = review_timestamp.groupby('review_score', as_index=False)['duration_arrived'].mean()
review_vs_arrive

#mean duration vs review
arrive_vs_review = review_timestamp.groupby('duration_arrived', as_index=False)['review_score'].mean()
arrive_vs_review.head(3)

#count arrive
arrive_review_count = review_timestamp.groupby('duration_arrived', as_index=False)['review_score'].count()
arrive_review_count.head(3)

sns.barplot(arrive_vs_review, x='duration_arrived', y='review_score')

sns.barplot(arrive_review_count, x='duration_arrived', y='review_score')

#too many anomalies in higher duration arrived, where the higher duration has too few sample

sns.barplot(review_vs_arrive, x='review_score', y='duration_arrived')

#Higher duration has negative impact on review (in general)

np.corrcoef(review_timestamp['duration_arrived'], review_timestamp['review_score'])

#Review Score has negative relationship with duration arrived

"""### Estimated Duration Accuracy"""

df_timestamp[df_timestamp['estimated_qos']<0]

(len(df_timestamp[df_timestamp['estimated_qos']<0])/len(df_timestamp))*100

#Error in the estimated duration is about 8.12%

"""### Traffic

#### Peak Hour
"""

df_peak_hour = df_timestamp.iloc[:, [0,3]]

df_peak_hour['hour'] = df_peak_hour['order_purchase_timestamp'].dt.hour
df_peak_hour

df_peak_hour = df_peak_hour.groupby('hour',as_index=False)['order_id'].count()
df_peak_hour.head(3)

plt.figure(figsize=(15,3))
ax = sns.barplot(df_peak_hour,
                 x='hour',
                 y='order_id',
                 palette='crest',
                 hue='order_id',
                 dodge=False)
ax.legend_.remove()

"""#### Peak Day of Week"""

df_time_price.head(5)

df_time_price['day_of_week'] = df_time_price['order_purchase_timestamp'].dt.dayofweek
df_time_price['day_name'] = df_time_price['order_purchase_timestamp'].dt.day_name()

df_traffic_dayofweek = df_time_price.groupby(['day_of_week', 'day_name'], as_index=False)['order_id'].nunique()
df_traffic_dayofweek

ax = sns.barplot(data=df_traffic_dayofweek,
            x='day_name',
            y='order_id',
            palette='crest',
            hue='order_id',
            dodge=False)
ax.legend_.remove()

"""#### Peak Date of Month"""

df_time_price['date'] = df_time_price['order_purchase_timestamp'].dt.day
df_time_price.head(5)

df_traffic_date = df_time_price.groupby(['date'], as_index=False)['order_id'].nunique()
df_traffic_date.head(3)

plt.figure(figsize=(10,5))
ax = sns.barplot(data=df_traffic_date,
            x='date',
            y='order_id',
            palette='crest',
            hue='order_id',
            dodge=False)
ax.legend_.remove()

#no particular trend

"""### Customer"""

#unique customer count location
df_customer_distinct = df_customer.drop_duplicates(subset=['customer_unique_id'])
df_customer_distinct.head(3)

#unique customer number of order location
df_order_location = df_delivery.merge(df_customer, on='customer_id', how='inner').iloc[:,[0,-2,-1]]
df_order_location.head(3)

"""#### State

##### By Unique Customer
"""

unique_customer_state = df_customer_distinct.groupby('customer_state', as_index=False)['customer_unique_id'].count().sort_values('customer_unique_id', ascending=False).reset_index(drop=True)
unique_customer_state.head(5)

#similar to revenue

"""##### By number of order"""

unique_order_state = df_order_location.groupby('customer_state', as_index=False)['order_id'].nunique().sort_values('order_id', ascending=False)
unique_order_state.head(5)

#similar to revenue & unique customer

"""### Payment"""

payment_order = df_timestamp[df_timestamp['order_status']=='delivered'].merge(df_order_payment, on='order_id', how='inner')
payment_order

"""#### Revenue"""

payment_revenue = payment_order.groupby('payment_type', as_index=False)['payment_value'].sum().sort_values(by='payment_value', ascending=False).reset_index(drop=True)
payment_revenue

"""#### Total Order"""

payment_count = payment_order.groupby('payment_type', as_index=False)['order_id'].count().sort_values(by='order_id', ascending=False).reset_index(drop=True)
payment_count

"""#### Pay Weight"""

payment_weight = payment_revenue['payment_value']/payment_count['order_id']
payment_weight

"""## Visualization & Explanatory Analysis

### Question 1: How is the efficient strategy to improve revenue?

#### Revenue growth
"""

plt.figure(figsize=(15,5))
sns.lineplot(data=df_time_price, x='order_purchase_month-year', y='payment_value', estimator='sum', ci=None)

"""While the revenue growth is outstanding from 2016 to 2018, we can see the decline in late 2018.

Our approach will try to solve this problem by analyzing revenue in each of these category to see its significance:
- Product category
- Area (state, city)

While we explore, we might find further pieces to enchance our analysis.

#### Revenue in each product category
"""

def million_formatter(x, pos):
    return "%.1f M" % (x/1E6)

palette_bar = sns.color_palette('crest', n_colors=10)
palette_bar.reverse()
labels = round(df_revenue_product['payment_value']/1E6, 2).head(10).astype('str') + 'M'

fig, ax = plt.subplots(figsize=(14,5))
sns.barplot(data=df_revenue_product.head(10), x='payment_value', y='product_category_name_english', palette=palette_bar)

ax.bar_label(ax.containers[0], labels=labels)
ax.xaxis.set_major_formatter(million_formatter)

"""- With the top 10 most generated revenue product, the profile of the buyer is pretty variative
- Even within the top 3 product, we can see different user segmentation based on its interest (household, beauty product, electronics)
- While the user is pretty variative, we can see mostly the interest in household furnitures (bed bath table, furniture decor, housewares)

#### Revenue in Each State & City
"""

top_5 = revenue_area_state[:5]
new_row = pd.DataFrame(data={
    'customer_state':['Others'],
    'payment_value':[revenue_area_state['payment_value'][5:].sum()],
    'percentage':[revenue_area_state['percentage'][5:].sum()]
})
pie_revenue_state = pd.concat([top_5, new_row]).reset_index(drop=True)
pie_revenue_state

#By State
def million_formatter(x, pos):
    return "%.1f M" % (x/1E6)
palette_bar = sns.color_palette('crest', n_colors=10)
palette_bar.reverse()
labels = round(revenue_area_state['payment_value']/1E6, 3).head(10).astype('str') + 'M'

fig, ax = plt.subplots(figsize=(15,5))

sns.barplot(data=revenue_area_state.head(10), x='payment_value', y='customer_state', palette=palette_bar)
for bars in ax.containers:
        ax.bar_label(bars, labels=labels)
ax.xaxis.set_major_formatter(million_formatter)

fig, ax = plt.subplots(figsize=(4,4))
ax.pie(pie_revenue_state['payment_value'], labels=pie_revenue_state['customer_state'], autopct='%1.1f%%')
plt.title('Revenue Contribution per State (%)')

"""Right away we can tell there is a massive different in generated revenue between 'SP' and the others.
- SP alone has more revenue than 5 combined latter states.
- We can group the states based on its generated revenue characteristics: SP -> RJ, MG -> RS, PR -> BA, SC -> GO, DF, ES
"""

#By City
revenue_area_city = revenue_area.groupby(['customer_city','customer_state'], as_index=False)['payment_value'].sum().sort_values('payment_value', ascending=False).reset_index(drop=True)

def million_formatter(x, pos):
    return "%.1f M" % (x/1E6)
palette_bar = sns.color_palette('crest', n_colors=10)
palette_bar.reverse()
labels = round(revenue_area_city['payment_value']/1E6, 3).head(10).astype('str') + 'M'

fig, ax = plt.subplots(figsize=(15,5))

sns.barplot(data=revenue_area_city.head(10),
            x='payment_value',
            y='customer_city',
            hue='customer_state',
            ci=None,
            dodge=False)
for bars in ax.containers:
        ax.bar_label(bars, labels=labels)
ax.xaxis.set_major_formatter(million_formatter)

"""This results is honestly surprising. Turns out only one city (sao paulo, rio de janeiro, etc..) dominated the generated revenues in each highest revenue state (SP, RJ, MG, etc..) while the rest characteristic of user in each city is similar to other states.

According to these results, we can improve the revenues based on the scopes:
- For sao paulo and rio de janeiro, we can maintain the revenues by providing offer for long term user in the popular product category and also providing survey about future interests;
- For the remaining city with low generated revenues, especially in the SP and RJ state nearby highest revenue, we can plan advertisement strategy via online or offline to reach new potential customer

#### Extra Action - Maintain the Revenue through Offer and Survey Based on Payment Type
"""

def million_formatter(x, pos):
    return "%.1f M" % (x/1E6)
palette_bar = sns.color_palette('crest', n_colors=4)
palette_bar.reverse()
labels = round(payment_revenue['payment_value']/1E6, 3).head(10).astype('str') + 'M'

fig, ax = plt.subplots(figsize=(9,2))

sns.barplot(data=payment_revenue,
            x='payment_value',
            y='payment_type',
            palette=palette_bar,
            ci=None
            )
for bars in ax.containers:
        ax.bar_label(bars, labels=labels)
ax.xaxis.set_major_formatter(million_formatter)

"""With this we know our focust on maintaining revenue besides location & product category, we can focus at providing offers and survey to the user using payment with credit card and maybe some boleto to test the water.

### Question 2: What action can be done to reduce cost of operation efficiently?

#### Traffic

What we mean by cost of operation here is mainly related to transaction traffic. The number of transaction traffic caused will decide the number of resources needed to supply the server. By knowing  the activity in transaction traffic, we can manage the resources in the server side efficiently.

##### Traffic by Hour
"""

df_time_price['hour'] = df_time_price['order_purchase_timestamp'].dt.hour
df_traffic_hour = df_time_price.groupby('hour', as_index=False)['order_id'].nunique()
df_traffic_hour['percentage'] = (df_traffic_hour['order_id']/df_traffic_hour['order_id'].sum())*100

plt.figure(figsize=(12,5))
ax = sns.barplot(data=df_traffic_hour,
                 x='hour',
                 y='order_id',
                 palette='crest',
                 hue='order_id',
                 dodge=False)
for i in ax.containers:
    ax.bar_label(i,)
ax.legend_.remove()

"""- By these graphic, we know the peak traffic is from 10.00 - 22.00 so the resources allocated in these hours need to be high;
- After 22.00, we can gradually reduce the resources until 05.00;
- After 05.00, we can gradually increase the resources until 10.00 where the resources is ready at the highest capacity

##### Traffic By Day of Week
"""

df_time_price['day_of_week'] = df_time_price['order_purchase_timestamp'].dt.dayofweek
df_time_price['day_name'] = df_time_price['order_purchase_timestamp'].dt.day_name()
df_traffic_dayofweek = df_time_price.groupby(['day_of_week', 'day_name'], as_index=False)['order_id'].nunique()

df_traffic_dayofweek

plt.figure(figsize=(7,3))
ax = sns.barplot(data=df_traffic_dayofweek,
            x='day_name',
            y='order_id',
            palette='crest',
            hue='order_id',
            dodge=False)
ax.legend_.remove()

"""From these results, we can cycle the server load priority from weekdays (high) to weekend (lower)

## Conclusion

1. Question 1 - How is the efficient strategy to improve revenue?

There are 2 strategies, increase the interests of active user and expand the userbase
- Active users are correlated to the area which generated the highest revenue since the user has similar economic activity
    - Focus in the states which generated the highest revenue especially SP which contributed 36% of total revenue
    - Look at the highest generated revenue city in each state
    - Survey about future interests and offer promo related to the highest generated revenue product category targeting user with the credit card payment type
- Since only 1 city dominated each generated revenue in each state, the next priority is to expand the business
    - Focus in the area nearby the city with highest generated revenue
    - Do some online and offline advertisement in city close to Sao Paulo since it should have similar economic activity
    - If the advertisement is successful, do another in city close to Rio de Janeiro

2. Question 2 - What action can be done to reduce cost of operation efficiently?

Resources on server load should be allocated dynamicly instead of static depending on the traffic, and here's how:
- Resources allocated to the server during weekdays should be higher than weekend
- Factoring the resources during day of week, there are 3 stages of allocated strategies during a day:
    - 10.00 - 22.00 : Peak traffic (highest server load)
    - 22.00 - 05.00 : High traffic -> Lowest traffic (lower the load)
    - 05.00 - 10.00 : Lowest traffic -> Peak traffic (start transitioning into the highest load)
"""